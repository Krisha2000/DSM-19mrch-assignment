{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43841446-4469-459a-93de-ac6b04e194b3",
   "metadata": {},
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e82e4f-072f-4eaf-856d-0da7603f130c",
   "metadata": {},
   "source": [
    "Q1. Min-Max scaling is a data preprocessing technique that scales the values of a feature to a fixed range, usually between 0 and 1. The formula for Min-Max scaling is:\n",
    "\n",
    "x_scaled = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "where x is the original value of the feature, x_min is the minimum value of the feature, and x_max is the maximum value of the feature.\n",
    "\n",
    "Min-Max scaling is useful in data preprocessing because it can transform the features to a common range, which can make it easier to compare them. It can also help to avoid the domination of features with larger values over features with smaller values in some machine learning models. For example, in image processing, pixel values can be scaled to a fixed range to improve the performance of neural networks.\n",
    "\n",
    "Example: Suppose you have a dataset of house prices, and the price of the houses ranges from $100,000 to $1,000,000. Min-Max scaling can be used to transform the price values to a common range between 0 and 1. If a house costs $500,000, the Min-Max scaling formula would give:\n",
    "\n",
    "x_scaled = ($500,000 - $100,000) / ($1,000,000 - $100,000) = 0.444\n",
    "\n",
    "So the scaled value of the house price is 0.444."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306af397-c066-4878-a9ea-5eaef7e1516e",
   "metadata": {},
   "source": [
    "# Quetion : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf362f7-a666-4bd6-a7ec-a444d95dfe49",
   "metadata": {},
   "source": [
    "The Unit Vector technique is another feature scaling technique that scales the values of a feature to have a length of 1. The formula for Unit Vector scaling is:\n",
    "\n",
    "x_scaled = x / ||x||\n",
    "\n",
    "where x is the original value of the feature, and ||x|| is the Euclidean length of the vector.\n",
    "\n",
    "The main difference between Min-Max scaling and Unit Vector scaling is that Min-Max scaling transforms the features to a fixed range, whereas Unit Vector scaling transforms the features to have the same length. Unit Vector scaling is useful when the magnitude of the feature values is important, but the actual values are not.\n",
    "\n",
    "Example: Suppose you have a dataset of customer ratings on a scale of 1 to 5, and you want to scale the ratings to have the same length. If a customer gave a rating of 4, the Unit Vector scaling formula would give:\n",
    "\n",
    "x_scaled = 4 / sqrt(4^2) = 0.894\n",
    "\n",
    "So the scaled value of the rating is 0.894."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa4ed7-a439-4bc3-9f4e-6ed59299d139",
   "metadata": {},
   "source": [
    "# Quetion : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084302a4-4c3c-4274-b194-376622d36b01",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a technique used in dimensionality reduction to reduce the number of features in a dataset while preserving the most important information. PCA finds a set of new features, called principal components, that are linear combinations of the original features and have the largest variance. The first principal component captures the most variance, the second principal component captures the second-most variance, and so on.\n",
    "\n",
    "PCA is used in dimensionality reduction because it can reduce the complexity of the data, remove noise, and improve the performance of machine learning models by reducing the number of features. PCA is commonly used in image and speech processing applications.\n",
    "\n",
    "Example: Suppose you have a dataset of customer orders, and the dataset contains 10 features, such as customer ID, order date, order total, etc. PCA can be used to reduce the number of features to, say, 3 principal components, which can capture most of the variance in the dataset. The new features will be linear combinations of the original features, and they will be orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f4d26-a2d9-4f50-9334-3b3cbb04dad0",
   "metadata": {},
   "source": [
    "# Quetion : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b4788-9d41-4725-b7d2-035cf0512c92",
   "metadata": {},
   "source": [
    " PCA and Feature Extraction are closely related concepts. Feature Extraction is the process of extracting important features from a dataset to reduce its dimensionality or to improve the performance of machine learning models. PCA can be used for Feature Extraction by extracting the principal components that capture the most variance in the dataset. The new features will be linear combinations of the original features, and they will be orthogonal to each other.\n",
    "\n",
    "Example: Suppose you have a dataset of customer orders, and the dataset contains 10 features, such as customer ID, order date, order total, etc. PCA can be used to reduce the number of features to, say, 3 principal components, which can capture most of the variance in the dataset. The new features will be linear combinations of the original features, and they will be orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521e8e4-f1af-4585-9c87-1613c4ec87ed",
   "metadata": {},
   "source": [
    "# Quetion : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294a2c9-eab5-434a-bd46-680868d9bcc2",
   "metadata": {},
   "source": [
    "In the case of a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data to ensure that all the features are on a common scale between 0 and 1. This is important because the features may have different scales, and some features may have a larger range of values than others. If the features are not scaled, some features may dominate the others, which can lead to biased recommendations.\n",
    "\n",
    "For example, suppose the dataset contains the following features for each food item: price (in dollars), rating (on a scale of 1 to 5), and delivery time (in minutes). To use Min-Max scaling, we would first find the minimum and maximum values for each feature, then scale the values using the following formula:\n",
    "\n",
    "x_scaled = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "where x is the original value of the feature, x_min is the minimum value of the feature, and x_max is the maximum value of the feature.\n",
    "\n",
    "After scaling, all the features will be on the same scale between 0 and 1, which can make it easier to compare them and make recommendations based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12beeb66-74e5-47b2-b042-0a106286d34a",
   "metadata": {},
   "source": [
    "# Quetion : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc213a-500a-421b-932d-ff69d1fd4b19",
   "metadata": {},
   "source": [
    "When working with a large dataset with many features, it can be difficult to analyze and model the data effectively. PCA can be a useful technique to reduce the dimensionality of the dataset while retaining most of the important information. Here's how you could use PCA to reduce the dimensionality of a stock price prediction dataset:\n",
    "\n",
    "First, you would need to standardize the features so that they have a mean of 0 and a standard deviation of 1. This is important because PCA is sensitive to the scale of the features, and standardizing them ensures that all the features are on the same scale.\n",
    "\n",
    "Next, you would apply PCA to the standardized dataset to extract the principal components. The number of principal components you would retain would depend on how much variance in the data you want to preserve. For example, if you retain 90% of the variance, then the number of principal components retained would be the minimum number needed to explain 90% of the variance in the data.\n",
    "\n",
    "After extracting the principal components, you would then use them as new features in your stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f309e-2b9f-4f80-ac5d-7b910f5d4f11",
   "metadata": {},
   "source": [
    "# Quetion : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9058eab-9533-418a-8330-6bb6b66e12d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "\n",
    "data_scaled = 2 * (data - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "print(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cb7e3-08a2-4c18-a120-09f718703d90",
   "metadata": {},
   "source": [
    "# Quetion : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5b5d5-5dbb-4da2-a357-2992713ec77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
